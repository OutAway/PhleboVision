{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OutAway/PhleboVision/blob/main/vggnet_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "xUt5AheXNtOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 183PvdHbFQgUO6cqaDzuOkTGcKUWlMtj3\n",
        "!gdown 1-S77hvJpwALtA3hJT0_KpHz3V6NKbDHB\n",
        "!unzip -o /content/dataset.zip\n",
        "!pip install requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlyZ423fOuaD",
        "outputId": "61433830-7797-47f2-ea23-f59d033bf79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=183PvdHbFQgUO6cqaDzuOkTGcKUWlMtj3\n",
            "To: /content/dataset.zip\n",
            "100% 26.9M/26.9M [00:00<00:00, 88.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-S77hvJpwALtA3hJT0_KpHz3V6NKbDHB\n",
            "To: /content/requirements.txt\n",
            "100% 8.55k/8.55k [00:00<00:00, 23.8MB/s]\n",
            "Archive:  /content/dataset.zip\n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series001_AdjustClr001_z010_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series002_AdjustClr002_z070_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series002_AdjustClr002_z071_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series003_AdjustClr002_z26_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series003_AdjustClr002_z27_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series005_AdjustClr001_z43_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series005_AdjustClr001_z44_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series006_AdjustClr001_z29_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series006_AdjustClr001_z38_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series038_AdjustClr001_z28_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series038_AdjustClr001_z29_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series049_AdjustClr001_z28_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series049_AdjustClr001_z29_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series058_AdjustClr001_z29_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series059_AdjustClr001_z36_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series073_AdjustClr001_z42_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series074_AdjustClr001_z22_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series083_AdjustClr001_z33_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series089_AdjustClr001_z183_ch00_SV.jpg  \n",
            "  inflating: dataset/acantopharynx/Project_New collection001_Series099_AdjustClr001_z036_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_1_Series001_z66_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_1_Series002_z27_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_1_Series003_z38_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_1_Series006_z38_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_2_Series008_z75_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_2_Series010_z07_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_2_Series011_z08_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_2_Series013_z27_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_3_Series016_z065_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_3_Series017_z21_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_3_Series018_z49_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_3_Series020_z02_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_3_Series026_z050_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_3_Series027_z75_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_3_Series030_z03_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_4_Series026_z010_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_4_Series027_z38_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_4_Series029_z56_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_4_Series030_z59_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_4_Series032_z23_ch00_SV.jpg  \n",
            "  inflating: dataset/atroclavata/Mi atroclavata IA_atro_5_Series037_z14_ch00_SV.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgWg6srYDSE"
      },
      "source": [
        "\n",
        "# VGGNET With Keras on Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXfwoAE2YDSG"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras import backend as K\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "#from keras.applications.imagenet_utils import _obtain_input_shape ## keras =< 2.2.0\n",
        "from keras.utils import get_source_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPXeoa7BYDSH"
      },
      "outputs": [],
      "source": [
        "def VGGupdated(input_tensor=None,classes=2):\n",
        "\n",
        "    img_rows, img_cols = 300, 300   # by default size is 224,224\n",
        "    img_channels = 3\n",
        "\n",
        "    img_dim = (img_rows, img_cols, img_channels)\n",
        "\n",
        "    img_input = Input(shape=img_dim)\n",
        "\n",
        "    # Block 1\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "    # Block 5\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "\n",
        "    # Classification block\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "    x = Dense(classes, activation='softmax', name='predictions')(x)\n",
        "\n",
        "    # Create model.\n",
        "\n",
        "\n",
        "    model = Model(inputs = img_input, outputs = x, name='VGGdemo')\n",
        "\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgmdGEhWYDSJ",
        "outputId": "b58043b5-ca45-48c6-ad22-5892b87c070d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['acantopharynx', 'atroclavata']\n",
            "Classes:  2\n",
            "acantopharynx: 20 entries\n",
            "atroclavata: 21 entries\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "root_dir = 'dataset'  # you need to define this\n",
        "dataset_cls = os.listdir(root_dir)\n",
        "\n",
        "class_types = os.listdir(root_dir)\n",
        "print (class_types)  # what kinds of rooms are in this dataset\n",
        "print(\"Classes: \", len(dataset_cls))\n",
        "for entry in class_types:\n",
        "    num_files = len(os.listdir(os.path.join(root_dir, entry)))\n",
        "    print(f\"{entry}: {num_files} entries\")\n",
        "\n",
        "entry_files = []  # I've changed the variable name to avoid confusion\n",
        "\n",
        "for entry in class_types:\n",
        "    # Get all the file names\n",
        "    all_entry = os.listdir(os.path.join(root_dir, entry))\n",
        "\n",
        "    # Add them to the list\n",
        "    for file in all_entry:\n",
        "        entry_files.append((entry, os.path.join(root_dir, entry, file)))\n",
        "#print(entry_files)  # this variable 'rooms' is not defined anywhere\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGGupdated(classes = len(dataset_cls)) # bedroom and diningroom\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "SuKXXAfDbF3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGGg3vojYDSL",
        "outputId": "76f33c91-7b27-4e0a-cfc6-472cbf8dc20d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         species                                               file\n",
            "0  acantopharynx  dataset/acantopharynx/Project_New collection00...\n",
            "1  acantopharynx  dataset/acantopharynx/Project_New collection00...\n",
            "2  acantopharynx  dataset/acantopharynx/Project_New collection00...\n",
            "3  acantopharynx  dataset/acantopharynx/Project_New collection00...\n",
            "4  acantopharynx  dataset/acantopharynx/Project_New collection00...\n",
            "        species                                               file\n",
            "36  atroclavata  dataset/atroclavata/Mi atroclavata IA_atro_4_S...\n",
            "37  atroclavata  dataset/atroclavata/Mi atroclavata IA_atro_5_S...\n",
            "38  atroclavata  dataset/atroclavata/Mi atroclavata IA_atro_2_S...\n",
            "39  atroclavata  dataset/atroclavata/Mi atroclavata IA_atro_2_S...\n",
            "40  atroclavata  dataset/atroclavata/Mi atroclavata IA_atro_4_S...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Build a dataframe\n",
        "phlebo_df = pd.DataFrame(data=entry_files, columns=['species', 'file'])\n",
        "print(phlebo_df.head())\n",
        "print(phlebo_df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUlblg8PYDSL",
        "outputId": "f8e967c9-8005-464a-85b3-8d56f08ccc3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rooms in the dataset:  41\n",
            "rooms in each category: \n",
            "atroclavata      21\n",
            "acantopharynx    20\n",
            "Name: species, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Let's check how many samples for each category are present\n",
        "print(\"Total number of rooms in the dataset: \", len(phlebo_df))\n",
        "\n",
        "room_count = phlebo_df['species'].value_counts()\n",
        "\n",
        "print(\"rooms in each category: \")\n",
        "print(room_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvj2SAPZYDSL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import cv2\n",
        "path = root_dir + '/'\n",
        "\n",
        "\n",
        "im_size = 300\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for i in dataset_cls:\n",
        "    data_path = path + str(i)\n",
        "    filenames = [i for i in os.listdir(data_path) ]\n",
        "\n",
        "    for f in filenames:\n",
        "        img = cv2.imread(data_path + '/' + f)\n",
        "        img = cv2.resize(img, (im_size, im_size))\n",
        "        images.append(img)\n",
        "        labels.append(i)\n",
        "\n",
        "#print(images)\n",
        "#print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92wRb50LYDSM",
        "outputId": "ce5260a8-0dab-46be-9b01-da6178313511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 300, 300, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "images = np.array(images)\n",
        "\n",
        "images = images.astype('float32') / 255.0\n",
        "images.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnxTUD59YDSM",
        "outputId": "f329f47b-83f3-432f-be02-312c62f36d46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
        "y=phlebo_df['species'].values\n",
        "#print(y[:5])\n",
        "\n",
        "y_labelencoder = LabelEncoder ()\n",
        "y = y_labelencoder.fit_transform (y)\n",
        "print (y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrq-l6m0YDSM",
        "outputId": "1e797214-b7ad-446f-80f3-3d18f3555973",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "y=y.reshape(-1,1)\n",
        "onehotencoder = OneHotEncoder()  # No need to specify categorical_features\n",
        "Y= onehotencoder.fit_transform(y)\n",
        "Y.shape  #(40, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufkUU__CYDSM",
        "outputId": "b33e7176-fe82-4f65-b6ec-23acb09d9604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38, 300, 300, 3)\n",
            "(38, 2)\n",
            "(3, 300, 300, 3)\n",
            "(3, 2)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import tensorflow as tf\n",
        "\n",
        "\n",
        "images, Y = shuffle(images, Y, random_state=1)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.05, random_state=415)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# flatten the labels if they're not already flat\n",
        "train_y = train_y.toarray()\n",
        "test_y = test_y.toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert only if they are in SparseTensor format\n",
        "if isinstance(train_x, tf.SparseTensor):\n",
        "    train_x = tf.sparse.to_dense(train_x)\n",
        "if isinstance(train_y, tf.SparseTensor):\n",
        "    train_y = tf.sparse.to_dense(train_y)\n",
        "if isinstance(test_x, tf.SparseTensor):\n",
        "    test_x = tf.sparse.to_dense(test_x)\n",
        "if isinstance(test_y, tf.SparseTensor):\n",
        "    test_y = tf.sparse.to_dense(test_y)\n",
        "\n",
        "# inspect the shape of the training and testing.\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "OCK1_mFl0GKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_x, train_y, epochs = 1, batch_size = 32)"
      ],
      "metadata": {
        "id": "j5YInZBeU58I",
        "outputId": "a7637770-6c6b-48a0-93cf-b7ecad4f9fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 179s 31s/step - loss: 3.3488 - accuracy: 0.4737\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 167s 33s/step - loss: 0.6934 - accuracy: 0.5263\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 169s 36s/step - loss: 0.6932 - accuracy: 0.5263\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 163s 33s/step - loss: 0.6917 - accuracy: 0.5263\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 161s 33s/step - loss: 0.6926 - accuracy: 0.5263\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 161s 31s/step - loss: 0.6935 - accuracy: 0.5263\n",
            "Epoch 7/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rRo6_op4YDSN"
      },
      "outputs": [],
      "source": [
        "preds = model.evaluate(test_x, test_y)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "#print (\"Test Accuracy = \" + str(preds[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlaZP3TwYDSN"
      },
      "source": [
        "# Take input from User and Classify that image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test from default database"
      ],
      "metadata": {
        "id": "kI49Uijs2Hz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 17wYgS56MUv9ZAkr3KY_wqurvJbTY6Kdv\n",
        "!unzip -o /content/test.zip"
      ],
      "metadata": {
        "id": "MPcgDOIs2ao1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = 'test'\n",
        "img_size = (300,300)\n",
        "files = os.listdir(test_path)"
      ],
      "metadata": {
        "id": "yKQt6BQY3lqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload own file"
      ],
      "metadata": {
        "id": "SUMsEwKG15Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "img_size = (300,300)\n",
        "\n",
        "# Load your model here.\n",
        "# model = ...\n",
        "\n",
        "dir_path = 'uploaded'\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "def load_and_process_image(img_path):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize(img_size)\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    print('Input image shape:', img.shape)\n",
        "    return img\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    global file_index\n",
        "    plt.figure(figsize=(6,6))\n",
        "    file_path = os.path.join(dir_path, uploaded_files[file_index])\n",
        "    img = load_and_process_image(file_path)\n",
        "\n",
        "    # Predict on the current image\n",
        "    print(model.predict(img))\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(np.squeeze(img, axis=0))\n",
        "    plt.show()\n",
        "\n",
        "    file_index += 1\n",
        "    if file_index >= len(uploaded_files):\n",
        "        file_index = 0\n",
        "\n",
        "uploaded = files.upload()  # This will prompt you to upload a file.\n",
        "\n",
        "uploaded_files = []\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "    # Save the uploaded file to the 'uploaded' directory\n",
        "    with open(os.path.join(dir_path, fn), 'wb') as f:\n",
        "        f.write(uploaded[fn])\n",
        "        uploaded_files.append(fn)\n",
        "\n",
        "file_index = 0\n",
        "\n",
        "button = widgets.Button(description=\"Next Image\")\n",
        "display(button)\n",
        "button.on_click(on_button_clicked)\n",
        "\n"
      ],
      "metadata": {
        "id": "ctLac6HL10Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWkhHIhFYDSN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from matplotlib.pyplot import imread, imshow\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_path = 'test'\n",
        "files = os.listdir(test_path)\n",
        "file_index = 0\n",
        "\n",
        "# Load and process the image\n",
        "def load_and_process_image(file_path):\n",
        "    img = load_img(file_path, target_size=(img_size))  # adjust the target size to the required dimensions\n",
        "    x = img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    print('Input image shape:', x.shape)\n",
        "    return x\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    global file_index\n",
        "    plt.figure(figsize=(6,6))\n",
        "    file_path = os.path.join(test_path, files[file_index])\n",
        "    x = load_and_process_image(file_path)\n",
        "\n",
        "    # Predict on the current image\n",
        "    print(model.predict(x))\n",
        "\n",
        "    # Display the image\n",
        "    my_image = imread(file_path)\n",
        "    imshow(my_image)\n",
        "    plt.show()\n",
        "\n",
        "    file_index += 1\n",
        "    if file_index >= len(files):\n",
        "        file_index = 0\n",
        "\n",
        "\n",
        "button = widgets.Button(description=\"Next Image\")\n",
        "display(button)\n",
        "button.on_click(on_button_clicked)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}